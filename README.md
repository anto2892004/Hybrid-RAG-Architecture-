
---

# ğŸ“Š Multi-Generator RAG with Hybrid Retrieval and Judge Evaluation

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline that enhances response accuracy and diversity using **multiple LLMs** and a **judging model**, built around hybrid retrieval of PCA-reduced vector chunks from company earnings call transcripts.

---

## ğŸ”§ Tech Stack

* **Language**: Python 3.10+
* **LLM API**: OpenAI-compatible models via Nexus API
* **Embedding Model**: `text-embedding-3-small`
* **Vector DB**: Pinecone (1024D embeddings, with namespaces)
* **Dimensionality Reduction**: PCA (1536 â†’ 1024)
* **LLMs Used for Generation**:

  * `gemini-2.5-flash`
  * `nova-micro`
  * `llama3-8b-8192`
* **Judge Model**: `gpt-4.1-nano`

---

## âš™ï¸ Project Pipeline

```text
User submits query
        â”‚
        â–¼
Hybrid Retrieval (PCA-reduced embeddings) from Pinecone
        â”‚
        â–¼
Top-K relevant chunks selected from 2 namespaces (150, 300 token sizes)
        â”‚
        â–¼
Each LLM receives the same query + top chunks and generates a response
        â”‚
        â–¼
All responses are evaluated by the judge model based on 4 metrics:
  â€¢ Relevance  â€¢ Factual Accuracy  â€¢ Completeness  â€¢ Clarity
        â”‚
        â–¼
Best answer is selected and all evaluations are displayed
```

---

## ğŸ“¦ Features

* âœ… **Hybrid Retriever**: Uses both 150-token and 300-token chunk namespaces.
* âœ… **PCA Optimization**: Reduces embedding size for faster, more efficient vector search.
* âœ… **Multi-LLM Generator**: Leverages the strengths of different LLMs to diversify answers.
* âœ… **Judge Evaluation**: Fair evaluation by GPT-4.1-nano using defined scoring metrics.
* âœ… **Modular Design**: Easily pluggable components for retrieval, generation, and judging.

---

## ğŸ§  Evaluation Metrics

Each LLM response is evaluated on a scale of **1 to 10** for:

* **Relevance**: How well the answer addresses the query.
* **Factual Accuracy**: Correctness of information based on the context chunks.
* **Completeness**: Whether all parts of the question are answered.
* **Clarity**: Coherence and readability of the explanation.

The best response is selected with a justification.

---

## ğŸ“ Directory Structure

```
RAG-project/
â”‚
â”œâ”€â”€ main.py                        # End-to-end pipeline
â”œâ”€â”€ config.py                      # API keys and config
â”‚
â”œâ”€â”€ retrievers/
â”‚   â””â”€â”€ hybrid_retriever.py       # PCA + Pinecone + hybrid namespace search
â”‚
â”œâ”€â”€ generators/
â”‚   â””â”€â”€ llm_router.py             # Multi-model LLM generation logic
â”‚
â”œâ”€â”€ evaluators/
â”‚   â””â”€â”€ judge_answers.py          # Judge model to evaluate responses
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ text_embedding_small.py   # Embedding via Nexus
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ pca_1536_to_1024.pkl      # Precomputed PCA model for dim reduction
```

---

## ğŸ—ƒï¸ Data & Chunking

* **Source**: Earnings call transcripts from top tech companies.
* **Chunk Sizes**: 150 and 300 tokens.
* **Overlap**: 30 and 50 tokens respectively.
* **Metadata stored**:

  * `company`, `date`, `source`, `chunk_index`, `chunk_size`, `text`

---

## ğŸš€ How to Run

```bash
# Set up virtual environment and install dependencies
pip install -r requirements.txt

# Add your config in config.py (API keys, index name)

# Run the pipeline
python main.py
```

---

## ğŸŒ Future Improvements

* [ ] Streamlit-based UI for interactive use
* [ ] Caching of judge responses
* [ ] Support for user-defined prompt templates
* [ ] Asynchronous generation for faster response

---

## ğŸ“ License

MIT License. You are free to use, modify, and share this project with attribution.

---
